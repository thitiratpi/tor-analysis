# -*- coding: utf-8 -*-
"""Stramlit V2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mcqvvxCz6e16mRUk6zExbM53ldd2Fj-N
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
import warnings
from docx import Document
import pdfplumber
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# --- CONFIGURATION ---
warnings.filterwarnings('ignore')
st.set_page_config(page_title="Wisesight TOR Analyzer", layout="wide", page_icon="üìä")

# --- CACHE MODEL ---
@st.cache_resource
def load_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# --- HELPER FUNCTIONS ---
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_number_of_users(text):
    text_lower = text.lower()
    patterns = [
        r'(\d+)\s*(?:users?|‡∏Ñ‡∏ô|‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ)',
        r'(?:users?|‡∏Ñ‡∏ô|‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ)[:\s]*(\d+)',
        r'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô(?:‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ|‡∏Ñ‡∏ô)[:\s]*(\d+)',
        r'(?:‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ|‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)[:\s]*(\d+)\s*(?:users?|‡∏Ñ‡∏ô|‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ)'
    ]
    found_numbers = []
    for pattern in patterns:
        matches = re.findall(pattern, text_lower)
        for match in matches:
            try:
                num = int(match)
                if 1 <= num <= 1000: found_numbers.append(num)
            except: pass
    return max(set(found_numbers), key=found_numbers.count) if found_numbers else 10

def extract_data_backward_days(text):
    text_lower = text.lower()
    patterns = {
        'month': [r'(\d+)\s*(?:‡πÄ‡∏î‡∏∑‡∏≠‡∏ô|month)', r'‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á[:\s]*(\d+)\s*(?:‡πÄ‡∏î‡∏∑‡∏≠‡∏ô|month)'],
        'year': [r'(\d+)\s*(?:‡∏õ‡∏µ|year)', r'‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á[:\s]*(\d+)\s*(?:‡∏õ‡∏µ|year)'],
        'day': [r'(\d+)\s*(?:‡∏ß‡∏±‡∏ô|day)', r'‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á[:\s]*(\d+)\s*(?:‡∏ß‡∏±‡∏ô|day)']
    }
    for pat in patterns['month']:
        for m in re.findall(pat, text_lower):
            if 1 <= int(m) <= 120: return int(m) * 30
    for pat in patterns['year']:
        for y in re.findall(pat, text_lower):
            if 1 <= int(y) <= 10: return int(y) * 365
    for pat in patterns['day']:
        for d in re.findall(pat, text_lower):
            if 1 <= int(d) <= 3650: return int(d)
    return 90

def extract_social_channels(text):
    text_lower = text.lower()
    channels = {k: 0 for k in ['facebook', 'instagram', 'twitter', 'line', 'tiktok', 'youtube', 'pantip', 'webchat', 'live_chat', 'application', 'email']}
    channel_patterns = {
        'facebook': [r'facebook', r'fb', r'‡πÄ‡∏ü‡∏ã‡∏ö‡∏∏‡πä‡∏Å'],
        'instagram': [r'instagram', r'ig'],
        'twitter': [r'twitter', r'x\.com', r'‡∏ó‡∏ß‡∏¥‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡πå'],
        'line': [r'\bline\b', r'‡πÑ‡∏•‡∏ô‡πå'],
        'tiktok': [r'tiktok'],
        'youtube': [r'youtube'],
        'webchat': [r'webchat', r'web\s*chat'],
        'live_chat': [r'live\s*chat'],
        'application': [r'application', r'mobile\s*app', r'‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô']
    }
    for ch, pats in channel_patterns.items():
        for pat in pats:
            if re.search(pat, text_lower):
                channels[ch] = 1
                break
    return channels

def extract_sentences_from_tor(text, min_length=10):
    text = clean_text(text)
    paragraphs = text.split('\n')
    sentences = []
    for paragraph in paragraphs:
        sents = re.split(r'[.!?]+\s+', paragraph)
        sentences.extend(sents)
    unique_sentences = []
    seen = set()
    for sent in sentences:
        sent = sent.strip()
        if (len(sent) >= min_length and not sent.isdigit() and sent.lower() != 'nan' and len(sent.split()) >= 3):
            if sent not in seen:
                seen.add(sent)
                unique_sentences.append(sent)
    return unique_sentences

def read_file_content(uploaded_file):
    content = ""
    try:
        if uploaded_file.name.endswith('.txt'):
            content = str(uploaded_file.read(), "utf-8")
        elif uploaded_file.name.endswith('.docx'):
            doc = Document(uploaded_file)
            content = "\n".join([p.text for p in doc.paragraphs])
        elif uploaded_file.name.endswith('.pdf'):
            with pdfplumber.open(uploaded_file) as pdf:
                for page in pdf.pages:
                    content += page.extract_text() + "\n"
    except Exception as e:
        st.error(f"Error reading file: {e}")
    return content

def analyze_tor_process(tor_text, sentences_df, model):
    sentences_df = sentences_df.dropna(subset=['Product', 'Category'])
    has_th = 'Sentence (TH)' in sentences_df.columns
    has_eng = 'Sentence (ENG)' in sentences_df.columns
    if has_th and has_eng:
        sentences_df = sentences_df[(sentences_df['Sentence (TH)'].notna()) | (sentences_df['Sentence (ENG)'].notna())]

    tor_sentences = extract_sentences_from_tor(tor_text)
    if not tor_sentences: return None, 0, []

    tor_embeddings = model.encode(tor_sentences)
    product_unique_matches = {}
    results = []

    progress_bar = st.progress(0)
    status_text = st.empty()
    total_rows = len(sentences_df)

    for idx, (i, row) in enumerate(sentences_df.iterrows()):
        if idx % 5 == 0:
            progress_bar.progress(min(idx / total_rows, 1.0))
            status_text.caption(f"Analyzing... {idx}/{total_rows}")

        product = row['Product']
        category = row['Category']
        sent_th = str(row.get('Sentence (TH)', ''))
        sent_eng = str(row.get('Sentence (ENG)', ''))
        target_sent = sent_eng if sent_eng and sent_eng != 'nan' else sent_th
        if not target_sent or target_sent == 'nan': continue

        target_embed = model.encode([target_sent])
        similarities = cosine_similarity(target_embed, tor_embeddings)[0]
        best_idx = np.argmax(similarities)
        score = ((similarities[best_idx] + 1) / 2) * 100
        matched = score >= 60.0

        results.append({
            'Product': product, 'Category': category, 'Requirement': target_sent,
            'Matched TOR Segment': tor_sentences[best_idx] if matched else "-",
            'Similarity (%)': round(score, 2), 'Matched': matched
        })

        if product not in product_unique_matches: product_unique_matches[product] = {}
        if best_idx not in product_unique_matches[product]:
             product_unique_matches[product][best_idx] = {'similarity': score, 'matched': matched}
        else:
            if score > product_unique_matches[product][best_idx]['similarity']:
                product_unique_matches[product][best_idx] = {'similarity': score, 'matched': matched}

    progress_bar.progress(1.0)
    status_text.empty()

    results_df = pd.DataFrame(results)
    product_scores = {}
    for product in results_df['Product'].unique():
        matches = product_unique_matches[product]
        unique_sims = [m['similarity'] for m in matches.values()]
        avg_sim = np.mean(unique_sims) if unique_sims else 0
        prod_res = results_df[results_df['Product'] == product]
        match_rate = (len(prod_res[prod_res['Matched']]) / len(prod_res)) * 100
        matched_cats = prod_res[prod_res['Matched']]['Category'].nunique()
        total_cats = prod_res['Category'].nunique()
        cat_coverage = (matched_cats / total_cats) * 100 if total_cats > 0 else 0
        overall = (avg_sim * 0.55) + (match_rate * 0.35) + (cat_coverage * 0.10)
        product_scores[product] = overall

    matched_products = [p for p, s in product_scores.items() if s > 40]
    if not matched_products and product_scores: matched_products = [max(product_scores, key=product_scores.get)]
    return results_df, len(tor_sentences), matched_products

def calculate_budget(product, requirements, pricing_df, addon_df):
    num_users = requirements['num_users']
    days = requirements['data_backward_days']
    pricing_df.columns = [str(c).strip() for c in pricing_df.columns]
    info = {}

    if product == 'Zocial Eye':
        pkgs = pricing_df[pricing_df['Product'] == 'Zocial Eye'].copy()
        suitable = pkgs[(pkgs['Data_Backward (Days)'] >= days) & (pkgs['User_Limit (User)'] >= num_users)]
        if suitable.empty: suitable = pkgs
        selected = suitable.sort_values('Total_Price_Per_Year (THB)').iloc[0]
        info = {
            'Product': 'Zocial Eye', 'Package': selected['Package'],
            'Base Price': selected['Total_Price_Per_Year (THB)'], 'Add-ons Cost': 0,
            'Total Price': selected['Total_Price_Per_Year (THB)'],
            'Details': f"Support {selected['User_Limit (User)']} Users, {selected['Data_Backward (Days)']} Days"
        }
    elif product == 'Warroom':
        pkgs = pricing_df[pricing_df['Product'] == 'Warroom'].copy()
        suitable = pkgs[pkgs['User_Limit (User)'] >= num_users]
        if suitable.empty: suitable = pkgs
        selected = suitable.sort_values('Total_Price_Per_Year (THB)').iloc[0]
        base_price = selected['Total_Price_Per_Year (THB)']
        connect_channels = requirements.get('warroom_connect_channels', 0)
        connect_cost = connect_channels * 60000
        extra_users = max(0, num_users - selected['User_Limit (User)'])
        extra_cost = extra_users * 12000
        total = base_price + connect_cost + extra_cost
        details = []
        if connect_cost > 0: details.append(f"Connect x{connect_channels}")
        if extra_cost > 0: details.append(f"Extra Users x{extra_users}")
        info = {
            'Product': 'Warroom', 'Package': selected['Package'],
            'Base Price': base_price, 'Add-ons Cost': connect_cost + extra_cost,
            'Total Price': total, 'Details': ", ".join(details) if details else "Base Package Only"
        }
    return info

# --- MAIN APP ---
try:
    model = load_model()
except Exception as e:
    st.error(f"Error loading Model: {e}"); st.stop()

st.title("üìë Wisesight TOR Analyzer & Budget")

with st.sidebar:
    st.header("üìÇ Upload Files")
    tor_file = st.file_uploader("Upload TOR", type=['pdf', 'docx', 'txt'])
    product_file = st.file_uploader("Product Reqs (Excel)", type=['xlsx'])
    pricing_file = st.file_uploader("Pricing (Excel)", type=['xlsx'])

if tor_file and product_file and pricing_file:
    tor_text = read_file_content(tor_file)
    req_df = pd.read_excel(product_file)
    xls = pd.ExcelFile(pricing_file)
    pricing_df = pd.read_excel(xls, 'Product_Pricing')
    addon_df = pd.read_excel(xls, 'AddOn_Feature')

    st.success("‚úÖ Files Loaded")
    if st.button("üöÄ Start Analysis", type="primary", use_container_width=True):
        with st.status("üîç Analyzing...", expanded=True) as status:
            reqs = {
                'num_users': extract_number_of_users(tor_text),
                'data_backward_days': extract_data_backward_days(tor_text),
                'channels': extract_social_channels(tor_text)
            }
            reqs['warroom_connect_channels'] = sum(reqs['channels'].get(ch,0) for ch in ['webchat', 'live_chat', 'application'])
            status.update(label="Complete!", state="complete", expanded=False)

        results_df, total_sents, matched_products = analyze_tor_process(tor_text, req_df, model)

        tab1, tab2 = st.tabs(["üèÜ Results & Budget", "üìã Details"])
        with tab1:
            st.success(f"Matched: {', '.join(matched_products)}")
            budget_list = []
            for prod in matched_products:
                b = calculate_budget(prod, reqs, pricing_df, addon_df)
                if b: budget_list.append(b)
            if budget_list:
                b_df = pd.DataFrame(budget_list)
                disp_df = b_df.copy()
                for c in ['Base Price', 'Add-ons Cost', 'Total Price']:
                    if c in disp_df.columns: disp_df[c] = disp_df[c].apply(lambda x: f"{x:,.0f}")
                st.dataframe(disp_df, use_container_width=True, hide_index=True)
                st.download_button("Download Budget", b_df.to_csv(index=False).encode('utf-8-sig'), "budget.csv")
        with tab2:
            st.dataframe(results_df, use_container_width=True)
            st.download_button("Download Analysis", results_df.to_csv(index=False).encode('utf-8-sig'), "analysis.csv")
else:
    st.info("Please upload all 3 files.")